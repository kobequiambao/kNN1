{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a5a6deb-3b06-4b1a-8084-eaec28995882",
   "metadata": {
    "id": "2a5a6deb-3b06-4b1a-8084-eaec28995882"
   },
   "source": [
    "# Neural Networks on Image Dataset\n",
    "\n",
    "In this notebook, we tackle a simple image classification dataset from sklearn: the digits dataset (handwritten digits images). For this exercise, we need the `torchvision` library. To install it, simply use the following installation command,\n",
    "\n",
    "```shell\n",
    "pip install torchvision\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05402ef6-38ca-47d4-8275-e76360f17f07",
   "metadata": {
    "id": "05402ef6-38ca-47d4-8275-e76360f17f07"
   },
   "source": [
    "## Instructions for All Labs\n",
    "* Read each cell and implement the TODOs sequentially. The markdown/text cells also contain instructions which you need to follow to get the whole notebook working.\n",
    "* Do not change the variable names unless the instructor allows you to.\n",
    "* Some markdown cells contain questions.\n",
    "  * For questions <span style=\"color:red;\">colored in red</span>, you must submit your answers in the corresponding Assignment in the course page. Make sure that you enter your responses in the item with the matching question code. Answers that do not follow the prescribed format will automatically be marked wrong by the checker.\n",
    "  * For questions <span style=\"color:green;\">colored in green</span>, you don't have to submit your answers, but you must think about these questions as they will help enrich your understanding of the concepts covered in the labs.\n",
    "* You are expected to search how to some functions work on the Internet or via the docs.\n",
    "* You may add new cells for \"scrap work\".\n",
    "* The notebooks will undergo a \"Restart and Run All\" command, so make sure that your code is working properly.\n",
    "* You may not reproduce this notebook or share them to anyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b3c645b-8dae-4d3d-b3e5-5fbcfde98fbe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b3c645b-8dae-4d3d-b3e5-5fbcfde98fbe",
    "outputId": "f7fe7a29-c750-4ae2-ebfd-efc05408c3a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21.0+cu126\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns  # pip install seaborn\n",
    "import torch\n",
    "import torchvision\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea4db3-1e53-4aa9-8be9-ed2caa1b1e8a",
   "metadata": {
    "id": "dbea4db3-1e53-4aa9-8be9-ed2caa1b1e8a"
   },
   "source": [
    "Set the seed for PRNG and determinism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a799b3ae-ab56-470d-8605-b04c1e2afb03",
   "metadata": {
    "id": "a799b3ae-ab56-470d-8605-b04c1e2afb03"
   },
   "outputs": [],
   "source": [
    "seed = 73\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4918d82-f392-4361-8b62-45a05df7c800",
   "metadata": {
    "id": "a4918d82-f392-4361-8b62-45a05df7c800"
   },
   "source": [
    "Define the function for plotting a given set of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02bd5d76-7497-4b10-9663-4c286b6c02b2",
   "metadata": {
    "id": "02bd5d76-7497-4b10-9663-4c286b6c02b2"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "\n",
    "def show(images: List) -> None:\n",
    "    _, axs = plt.subplots(ncols=len(images), squeeze=False)\n",
    "    for index, image in enumerate(images):\n",
    "        image = image.detach()\n",
    "        axs[0, index].imshow(np.asarray(image), cmap=\"gray\")\n",
    "        axs[0, index].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31319ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu126\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  # Should show a version with 'cu121'\n",
    "print(torch.cuda.is_available())  # Should print True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8831a50-b6de-4cdc-a643-8161d7874082",
   "metadata": {
    "id": "d8831a50-b6de-4cdc-a643-8161d7874082"
   },
   "source": [
    "Load the digits dataset which consists of the features $X$ and labels $y$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccba4258-2a89-4195-95a9-3e10ca5dabf8",
   "metadata": {
    "id": "ccba4258-2a89-4195-95a9-3e10ca5dabf8"
   },
   "outputs": [],
   "source": [
    "images, labels = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c6768e-5078-4618-9d6e-d7213117d09c",
   "metadata": {
    "id": "b9c6768e-5078-4618-9d6e-d7213117d09c"
   },
   "source": [
    "Check the shape of the images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08e69f1d-bf7b-461a-bff0-c4e7810cac1a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08e69f1d-bf7b-461a-bff0-c4e7810cac1a",
    "outputId": "7e2963dd-d8c7-4293-bb89-9a11ec4f57ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fea3e3f-0df3-46a1-8672-1d411a88cda9",
   "metadata": {
    "id": "3fea3e3f-0df3-46a1-8672-1d411a88cda9"
   },
   "source": [
    "Check samples of the images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20040e1e-b21b-462e-91f2-ffe981546d09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20040e1e-b21b-462e-91f2-ffe981546d09",
    "outputId": "88b151dc-fc65-46bd-d831-7e600c6a6379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      "  15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "   0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "   0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      " [ 0.  0.  0. 12. 13.  5.  0.  0.  0.  0.  0. 11. 16.  9.  0.  0.  0.  0.\n",
      "   3. 15. 16.  6.  0.  0.  0.  7. 15. 16. 16.  2.  0.  0.  0.  0.  1. 16.\n",
      "  16.  3.  0.  0.  0.  0.  1. 16. 16.  6.  0.  0.  0.  0.  1. 16. 16.  6.\n",
      "   0.  0.  0.  0.  0. 11. 16. 10.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(images[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f80ce7e-30cb-454b-85dd-3944e9dced99",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6f80ce7e-30cb-454b-85dd-3944e9dced99",
    "outputId": "fbaca842-fd3e-4599-85d1-445b854923b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(labels[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8387f32e-b777-4f00-a60d-153d3f3b5c32",
   "metadata": {
    "id": "8387f32e-b777-4f00-a60d-153d3f3b5c32"
   },
   "source": [
    "From here, we can see that there are 1,797 instances of images and labels. However, we can see that each \"image\" has 1x64 vectors.\n",
    "\n",
    "Meanwhile, an image is supposed to have a width x length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e4a9f6a-a8b7-4dcb-9545-50709c84b906",
   "metadata": {
    "id": "3e4a9f6a-a8b7-4dcb-9545-50709c84b906"
   },
   "outputs": [],
   "source": [
    "# to do: reshape the `images` tensor to be of `num_instances x width x length` shape\n",
    "images = torch.tensor(images, dtype=torch.float32)\n",
    "num_instances = images.shape[0]\n",
    "images = images.view(num_instances, 8, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e123e8e9-9c14-4a94-ada7-78467268ece2",
   "metadata": {
    "id": "e123e8e9-9c14-4a94-ada7-78467268ece2"
   },
   "source": [
    "Retrieve the only first 5 images for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51b212e3-9c93-4e48-9990-a95820e6f7c1",
   "metadata": {
    "id": "51b212e3-9c93-4e48-9990-a95820e6f7c1"
   },
   "outputs": [],
   "source": [
    "# to do: retrieve the first 5 images\n",
    "image_grid = torchvision.utils.make_grid(images[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6dc487-fe82-4c56-a588-d8e6f4db8826",
   "metadata": {
    "id": "6f6dc487-fe82-4c56-a588-d8e6f4db8826"
   },
   "source": [
    "Check that we got the first 5 items and that the images have been properly reshaped. It should have a shape of 5x8x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d30b166b-09cb-4a77-a660-f748023b78a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d30b166b-09cb-4a77-a660-f748023b78a8",
    "outputId": "fdddfba6-48ee-44c7-8df0-c5a9447dbc5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_grid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b14b542-a5fe-461d-a7fc-6dc7646284c2",
   "metadata": {
    "id": "1b14b542-a5fe-461d-a7fc-6dc7646284c2"
   },
   "source": [
    "Each image having a shape of 8x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34db90a7-27d7-42a5-be14-d0c14eaf5e5b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34db90a7-27d7-42a5-be14-d0c14eaf5e5b",
    "outputId": "2043ca0e-ccb7-48fb-be9b-a4dc0fe1021d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_grid[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2329fac0-887e-4486-b52e-e1d658baf2ce",
   "metadata": {
    "id": "2329fac0-887e-4486-b52e-e1d658baf2ce"
   },
   "source": [
    "Plot the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2118f19-7427-4f09-a586-a3c586874877",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "e2118f19-7427-4f09-a586-a3c586874877",
    "outputId": "42dbec55-efa7-41bd-c644-65e54c772f1d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(image_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bb040b-cd65-4864-be8b-66a7f10e8df2",
   "metadata": {
    "id": "67bb040b-cd65-4864-be8b-66a7f10e8df2"
   },
   "source": [
    "Confirm the labels of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dd37399-db92-43ed-bf08-c0ace0fa2873",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dd37399-db92-43ed-bf08-c0ace0fa2873",
    "outputId": "5f52faa0-02a5-4356-d4c1-65e3f86152e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# to do: print the labels\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa522d7-9ce8-4d7b-889f-7f3427020f4f",
   "metadata": {
    "id": "6aa522d7-9ce8-4d7b-889f-7f3427020f4f"
   },
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "We split the images and labels to training/validation/test sets. The splitting is as follows,\n",
    "\n",
    "* Training: 80%\n",
    "* Test: 20%\n",
    "\n",
    "Then we get the 20% of the training set as the validation set, leaving us with the following,\n",
    "\n",
    "* Training: 64%\n",
    "* Validation: 16%\n",
    "* Test: 20%\n",
    "\n",
    "Recall that we reshaped our images to be of $num\\_instances \\times width \\times length$ shape.\n",
    "\n",
    "So, we have to reshape it back to $num\\_instances \\times dimensions$ before splitting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa91b85e-5f19-4769-9e82-95f0c4fbe933",
   "metadata": {
    "id": "aa91b85e-5f19-4769-9e82-95f0c4fbe933"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "images = images.reshape(images.shape[0], -1)\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    images, labels, test_size=2e-1, stratify=labels, shuffle=True, random_state=seed\n",
    ")\n",
    "\n",
    "train_features, validation_features, train_labels, validation_labels = train_test_split(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    test_size=2e-1,\n",
    "    stratify=train_labels,\n",
    "    shuffle=True,\n",
    "    random_state=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb333ce-5d9d-4021-a377-e3bb25962b99",
   "metadata": {
    "id": "4cb333ce-5d9d-4021-a377-e3bb25962b99"
   },
   "source": [
    "Confirm the dataset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26b69ef3-a453-4458-aecd-4f55281e3fdc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26b69ef3-a453-4458-aecd-4f55281e3fdc",
    "outputId": "2af105af-8a19-498f-ca7e-3083509a6850"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "\tTraining: 1149, Validation 288, Test: 360\n"
     ]
    }
   ],
   "source": [
    "# to do: print the dataset sizes\n",
    "print(f\"Dataset sizes:\")\n",
    "print(\n",
    "    f\"\\tTraining: {len(train_features)}, Validation {len(validation_features)}, Test: {len(test_features)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c0b8a-f279-4cdb-aef6-73bc9af81595",
   "metadata": {
    "id": "201c0b8a-f279-4cdb-aef6-73bc9af81595"
   },
   "source": [
    "<span style=\"color:red;\">**Question 6-19:** How many instances are there in the training, validation, and test sets respectively?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0113a54-3502-402a-98ec-4f42af80376c",
   "metadata": {
    "id": "201c0b8a-f279-4cdb-aef6-73bc9af81595"
   },
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c657da0-3b65-4e5a-9b58-2a0201c67c72",
   "metadata": {
    "id": "1c657da0-3b65-4e5a-9b58-2a0201c67c72"
   },
   "source": [
    "After splitting the dataset into training/validation/test sets, we can now pack them into PyTorch tensor objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40f189ab-c806-478d-8e96-2a1aa4bb769a",
   "metadata": {
    "id": "40f189ab-c806-478d-8e96-2a1aa4bb769a"
   },
   "outputs": [],
   "source": [
    "train_features = torch.tensor(train_features) if not isinstance(train_features, torch.Tensor) else train_features\n",
    "train_labels = torch.tensor(train_labels) if not isinstance(train_labels, torch.Tensor) else train_labels\n",
    "validation_features = torch.tensor(validation_features) if not isinstance(validation_features, torch.Tensor) else validation_features\n",
    "validation_labels = torch.tensor(validation_labels) if not isinstance(validation_labels, torch.Tensor) else validation_labels\n",
    "test_features = torch.tensor(test_features) if not isinstance(test_features, torch.Tensor) else test_features\n",
    "test_labels = torch.tensor(test_labels) if not isinstance(test_labels, torch.Tensor) else test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878e73e2-8e4c-4c8c-af2b-494a5acdea6f",
   "metadata": {
    "id": "878e73e2-8e4c-4c8c-af2b-494a5acdea6f"
   },
   "source": [
    "For these data to be ingestible for our model, we have to pack them into a [TensorDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d27581a8-9b3c-437b-ad6e-c00510ead45c",
   "metadata": {
    "id": "d27581a8-9b3c-437b-ad6e-c00510ead45c"
   },
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "\n",
    "\n",
    "# to do: pack the validation and test features and labels into a TensorDataset\n",
    "validation_dataset = torch.utils.data.TensorDataset(validation_features, validation_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed870926-8d0f-4cd6-beaa-2df33df8119d",
   "metadata": {
    "id": "ed870926-8d0f-4cd6-beaa-2df33df8119d"
   },
   "source": [
    "To automate the batching of the datasets during training, we pack them into [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5764a1c4-1595-483b-bd9c-9b7b8675f826",
   "metadata": {
    "id": "5764a1c4-1595-483b-bd9c-9b7b8675f826"
   },
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "\n",
    "\n",
    "# to do: pack the validation and test features and labels into a TensorDataset\n",
    "validation_dataset = torch.utils.data.TensorDataset(validation_features, validation_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "# define the batch size\n",
    "batch_size = 64\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=multiprocessing.cpu_count(),\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    dataset=validation_dataset,\n",
    "    batch_size=len(validation_dataset),\n",
    "    num_workers=multiprocessing.cpu_count(),\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=len(test_dataset),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec46a347-b4ab-448e-b3e7-d395af3059cd",
   "metadata": {
    "id": "ec46a347-b4ab-448e-b3e7-d395af3059cd"
   },
   "source": [
    "## Image Classifier (baseline)\n",
    "\n",
    "Implement an image classifier neural network with the following specifications,\n",
    "\n",
    "* 2 layers with each layer having 500 neurons each\n",
    "* Each hidden layer will use logistic/sigmoid function as its activation function\n",
    "* Use SGD as the optimization algorithm with learning rate $1 \\times 10^{-3}$\n",
    "\n",
    "Train for 50 epochs with mini-batch size 100. This will serve as our baseline model, which uses logistic/sigmoid function to learn the nonlinear relationships in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a81b523-01dd-4c1d-8722-85ddd7b82861",
   "metadata": {
    "id": "7a81b523-01dd-4c1d-8722-85ddd7b82861"
   },
   "outputs": [],
   "source": [
    "class ClassifierA(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # to do: define the hidden layers\n",
    "        self.hidden1 = torch.nn.Linear(64, 500)\n",
    "        self.hidden2 = torch.nn.Linear(500, 500)\n",
    "       \n",
    "        # to do: define the classification layer\n",
    "        self.output = torch.nn.Linear(500, 10)\n",
    "\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        # to do: define the forward pass function\n",
    "        x = features.view(features.shape[0], -1)\n",
    "        x = torch.sigmoid(self.hidden1(x))\n",
    "        x = torch.sigmoid(self.hidden2(x))\n",
    "        logits = self.output(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056e7ec6-3290-418e-9797-240e657d6426",
   "metadata": {
    "id": "056e7ec6-3290-418e-9797-240e657d6426"
   },
   "source": [
    "For convenience, we define a training function for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ef131a1-b8fc-4314-a986-81ea6cb7e60e",
   "metadata": {
    "id": "3ef131a1-b8fc-4314-a986-81ea6cb7e60e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: torch.nn.Module,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    validation_loader: torch.utils.data.DataLoader,\n",
    "    epochs: int = 50,\n",
    "    learning_rate: float = 1e-2,\n",
    "    display_interval: int = 5\n",
    ") -> Tuple[List, List, List, List]:\n",
    "\n",
    "    # to do:\n",
    "    # set the learning rate, pass the model parameters to be optimized\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_acc = list()\n",
    "    train_loss = list()\n",
    "    validation_acc = list()\n",
    "    validation_loss = list()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        epoch_loss = list()\n",
    "        epoch_acc = list()\n",
    "        \n",
    "        for features_batch, labels_batch in train_loader:\n",
    "            # to do: zero out the gradients to prevent gradient accumulation\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # to do: switch the features to GPU\n",
    "            features_batch = features_batch.cuda()\n",
    "            labels_batch = labels_batch.cuda().long()\n",
    "\n",
    "            # to do: compute model predictions\n",
    "            outputs = model(features_batch)\n",
    "\n",
    "            # to do: compute the loss\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "\n",
    "            # to do: compute accuracy of predictions in [0, 1] range\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            acc = (preds == labels_batch).float().mean().item()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            train_acc.append(acc)\n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_acc.append(acc)\n",
    "\n",
    "            # to do: run backprop\n",
    "            loss.backward()\n",
    "\n",
    "            # to do: optimize the weights based on backpropagated gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # to do: run model in inference mode\n",
    "                model.eval()\n",
    "                for features_batch, labels_batch in validation_loader:\n",
    "                    # to do: switch the features and labels to GPU\n",
    "                    features_batch = features_batch.cuda()\n",
    "                    labels_batch = labels_batch.cuda().long()\n",
    "\n",
    "                    # to do: compute the model predictions\n",
    "                    outputs = model(features_batch)\n",
    "\n",
    "                    # to do: compute the loss\n",
    "                    loss = criterion(outputs, labels_batch)\n",
    "\n",
    "                    # to do: compute the model predictions accuracy in [0, 1] range\n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    acc = (preds == labels_batch).float().mean().item()\n",
    "\n",
    "                    validation_loss.append(loss.item())\n",
    "                    validation_acc.append(acc)\n",
    "\n",
    "            # to do: enable training mode again\n",
    "            model.train()\n",
    "\n",
    "        if (epoch + 1) % display_interval == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "            print(\n",
    "                f\"\\tTraining Loss:  {np.mean(epoch_loss):.4f}, Training Acc: {np.mean(epoch_acc):.4f}\"\n",
    "            )\n",
    "            print(f\"\\tValidation Loss: {loss.item():.4f}, Validation Acc: {acc:.4f}\")\n",
    "\n",
    "    return train_acc, train_loss, validation_acc, validation_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f9c19e-5354-407d-8c5f-b2cc0d87ca95",
   "metadata": {
    "id": "20f9c19e-5354-407d-8c5f-b2cc0d87ca95"
   },
   "source": [
    "We also prepare an evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e59991d4-38b9-456f-a804-7d6d70acf410",
   "metadata": {
    "id": "e59991d4-38b9-456f-a804-7d6d70acf410"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: torch.nn.Module, data_loader: torch.utils.data.DataLoader\n",
    ") -> float:\n",
    "    # to do: disable gradient computation\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # to do: set model for pure eval\n",
    "        model.eval()\n",
    "\n",
    "        # to do: switch to CPU for computations\n",
    "        model.cpu()\n",
    "\n",
    "        for features_batch, labels_batch in data_loader:\n",
    "            # to do: compute the model predictions\n",
    "            outputs = model(features_batch)\n",
    "\n",
    "            # to do: compute the accuracy of the predictions in [0, 1]-range\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            acc = (preds == labels_batch).float().mean().item()\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3740b910-9228-42a8-a0d4-ef9579b67da3",
   "metadata": {
    "id": "3740b910-9228-42a8-a0d4-ef9579b67da3"
   },
   "source": [
    "Initialize the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcb7239a-b632-4088-896f-112e460efaef",
   "metadata": {
    "id": "bcb7239a-b632-4088-896f-112e460efaef"
   },
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "baseline_model = ClassifierA()\n",
    "\n",
    "# switch to GPU for computation\n",
    "model = baseline_model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eb3708-26f0-4296-bcb0-c442a8669461",
   "metadata": {
    "id": "84eb3708-26f0-4296-bcb0-c442a8669461"
   },
   "source": [
    "<span style=\"color:red;\">**Question 6-20:** What is the summation of the initial weight parameters for class 0 in the `baseline_model`?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10438ed5-34a2-4e97-ae21-2401a69b96b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10438ed5-34a2-4e97-ae21-2401a69b96b6",
    "outputId": "630df493-cd36-4140-c15a-7ef1510cfb33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summation of initial weights for class 0: -0.11486735939979553\n"
     ]
    }
   ],
   "source": [
    "final_layer_weights = baseline_model.output.weight\n",
    "\n",
    "sum_class_0 = final_layer_weights[0].sum().item()\n",
    "print(f\"Summation of initial weights for class 0: {sum_class_0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb61e40e-2e82-4659-b44d-5fd6695be2bc",
   "metadata": {
    "id": "cb61e40e-2e82-4659-b44d-5fd6695be2bc"
   },
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ff46f5-c0da-4992-be93-36f69974d65e",
   "metadata": {
    "id": "81ff46f5-c0da-4992-be93-36f69974d65e"
   },
   "source": [
    "<span style=\"color:red;\">**Question 6-21:** What is the summation of the initial weight parameters for class 9 in the `baseline_model`?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae52314f-cf8f-4851-96f5-7ebdbcd64bc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ae52314f-cf8f-4851-96f5-7ebdbcd64bc3",
    "outputId": "98fc3a7a-8e6a-4dbf-e0de-e0ec04fc0af2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summation of initial weights for class 9: -1.8027721643447876\n"
     ]
    }
   ],
   "source": [
    "sum_class_9 = final_layer_weights[9].sum().item()\n",
    "print(f\"Summation of initial weights for class 9: {sum_class_9}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dac908-9e3d-4b4d-bd64-8edd1473eb30",
   "metadata": {
    "id": "96dac908-9e3d-4b4d-bd64-8edd1473eb30"
   },
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac977192-4a53-4da3-abfc-28aafa4f55d7",
   "metadata": {
    "id": "ac977192-4a53-4da3-abfc-28aafa4f55d7"
   },
   "source": [
    "Set the number of epochs and learning rate to be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75b8ccfc-9e64-4051-956d-c7df39b19434",
   "metadata": {
    "id": "75b8ccfc-9e64-4051-956d-c7df39b19434"
   },
   "outputs": [],
   "source": [
    "# to do: set the epochs and the learning rate\n",
    "epochs = 40\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b825ce4-0704-4164-9e5e-ad8e70cc6e2a",
   "metadata": {
    "id": "4b825ce4-0704-4164-9e5e-ad8e70cc6e2a"
   },
   "source": [
    "Now, invoke the training function from earlier, to train the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db864674-c3ab-4b06-b29b-17e3bc0fd1cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db864674-c3ab-4b06-b29b-17e3bc0fd1cb",
    "outputId": "6435985b-cbb2-487b-c693-388272bb1541"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 13120, 7724, 12140, 8468, 8676, 1244, 9272, 3568, 11828, 6608, 8476, 12172) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kobeq\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1251\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1251\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\kobeq\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\queue.py:212\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m (\n\u001b[0;32m      2\u001b[0m     baseline_train_acc,\n\u001b[0;32m      3\u001b[0m     baseline_train_loss,\n\u001b[0;32m      4\u001b[0m     baseline_validation_acc,\n\u001b[0;32m      5\u001b[0m     baseline_validation_loss,\n\u001b[1;32m----> 6\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 57\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, validation_loader, epochs, learning_rate, display_interval)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# to do: run model in inference mode\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 57\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# to do: switch the features and labels to GPU\u001b[39;49;00m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeatures_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kobeq\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\kobeq\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1458\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1458\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kobeq\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1410\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1409\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1410\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1411\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1412\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\kobeq\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1264\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1263\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1265\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1266\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 13120, 7724, 12140, 8468, 8676, 1244, 9272, 3568, 11828, 6608, 8476, 12172) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "(\n",
    "    baseline_train_acc,\n",
    "    baseline_train_loss,\n",
    "    baseline_validation_acc,\n",
    "    baseline_validation_loss,\n",
    ") = train_model(\n",
    "    model=baseline_model,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=validation_loader,\n",
    "    epochs=epochs,\n",
    "    learning_rate=learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551ada2c-e04e-47e5-9394-a86a6824e5ac",
   "metadata": {
    "id": "551ada2c-e04e-47e5-9394-a86a6824e5ac"
   },
   "source": [
    "Let's see how did the model perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28731d86-cb6b-41ae-b005-956df77cfb55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28731d86-cb6b-41ae-b005-956df77cfb55",
    "outputId": "d4cb6042-9690-4d2b-f0a6-4881f22ac33f"
   },
   "outputs": [],
   "source": [
    "baseline_test_acc = evaluate_model(model=baseline_model, data_loader=test_loader)\n",
    "\n",
    "print(f\"Baseline model test accuracy: {baseline_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40256549-fb0e-4eb8-b6e8-73953f21911e",
   "metadata": {
    "id": "40256549-fb0e-4eb8-b6e8-73953f21911e"
   },
   "source": [
    "<span style=\"color:red;\">**Question 6-22:** What was the validation loss at epoch 40? What is the test accuracy of the `baseline_model`?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5549c05a-f9dc-4d1a-a97d-708cc1925a9c",
   "metadata": {
    "id": "5549c05a-f9dc-4d1a-a97d-708cc1925a9c"
   },
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4306ed02-37ef-4ebe-a3d4-ae7df4f52c83",
   "metadata": {
    "id": "4306ed02-37ef-4ebe-a3d4-ae7df4f52c83"
   },
   "source": [
    "## Image Classifier (challenger/experimental)\n",
    "\n",
    "It has been more than a decade that we know the ReLU activation function significantly improves the performance of a neural network over the logistic/sigmoid function.\n",
    "However, instead of simply believing the literature, we empirically explore that notion in this notebook.\n",
    "\n",
    "Implement an image classifier neural network with the following specifications,\n",
    "\n",
    "* 2 layers with each layer having 500 neurons each\n",
    "* Each hidden layer will use ReLU function as its activation function\n",
    "* Use SGD as the optimization algorithm with learning rate $1 \\times 10^{-3}$\n",
    "\n",
    "Train for 50 epochs with mini-batch size 100. This will serve as our challenger or experimental model.\n",
    "\n",
    "Notice that the only change we introduced was the usage of ReLU activation, all other design choices remain the same. This affords us a more \"apples-to-apples\" comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33c0cb33-4dcc-47fb-84e5-9c51b5001cc6",
   "metadata": {
    "id": "33c0cb33-4dcc-47fb-84e5-9c51b5001cc6"
   },
   "outputs": [],
   "source": [
    "class ClassifierB(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # to do: define the hidden layers\n",
    "        self.hidden1 = torch.nn.Linear(in_features=64, out_features=500)\n",
    "        self.hidden2 = torch.nn.Linear(in_features=500, out_features=500)\n",
    "\n",
    "        # to do: define the classification layer\n",
    "        self.output = torch.nn.Linear(in_features=500, out_features=10)\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        # to do: define the forward propagation\n",
    "        x = torch.relu(self.hidden1(features))\n",
    "        x = torch.relu(self.hidden2(x))\n",
    "        logits = self.output(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2d226-a88f-4f3f-b517-fd36b8306601",
   "metadata": {
    "id": "0be2d226-a88f-4f3f-b517-fd36b8306601"
   },
   "source": [
    "Initialize the experimental model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c55d48ab-d54c-4286-b390-9fe741ac6c78",
   "metadata": {
    "id": "c55d48ab-d54c-4286-b390-9fe741ac6c78"
   },
   "outputs": [],
   "source": [
    "# to do: initialize the experimental model\n",
    "experimental_model = ClassifierB()\n",
    "\n",
    "# to do: switch to GPU for computations\n",
    "experimental_model = experimental_model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192cb1df-1f8c-49f2-b031-04777eba8dae",
   "metadata": {
    "id": "192cb1df-1f8c-49f2-b031-04777eba8dae"
   },
   "source": [
    "Train the experimental model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7917cf8f-176b-4546-ae4a-28d4483ad9b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7917cf8f-176b-4546-ae4a-28d4483ad9b2",
    "outputId": "14e8be99-c8b5-455d-f4fd-7c3d8f3a42a3"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    experimental_train_acc,\n",
    "    experimental_train_loss,\n",
    "    experimental_validation_acc,\n",
    "    experimental_validation_loss,\n",
    ") = train_model(\n",
    "    model=experimental_model,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=validation_loader,\n",
    "    epochs=epochs,\n",
    "    learning_rate=learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e140beb-a18a-4071-b330-6046245fbe54",
   "metadata": {
    "id": "1e140beb-a18a-4071-b330-6046245fbe54"
   },
   "source": [
    "<span style=\"color:red;\">**Question 6-23:** What was the training loss and validation loss at epoch 35?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6fe90a-3915-4373-8b20-b489979a26e2",
   "metadata": {
    "id": "9a6fe90a-3915-4373-8b20-b489979a26e2"
   },
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc19bb2c-e388-4ea0-81c0-9e7f9c97e740",
   "metadata": {
    "id": "fc19bb2c-e388-4ea0-81c0-9e7f9c97e740"
   },
   "source": [
    "Evaluate the trained experimental model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e2f07-fa6d-4355-94b0-82a5cfec4994",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd9e2f07-fa6d-4355-94b0-82a5cfec4994",
    "outputId": "f377751f-9b09-46e4-85e4-39fcee766f15"
   },
   "outputs": [],
   "source": [
    "experimental_test_acc = evaluate_model(\n",
    "    model=experimental_model, data_loader=test_loader\n",
    ")\n",
    "\n",
    "print(f\"Experimental model test accuracy: {experimental_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84fb71d-a9fe-4b08-ab4a-af377ca9d1bf",
   "metadata": {
    "id": "b84fb71d-a9fe-4b08-ab4a-af377ca9d1bf"
   },
   "source": [
    "<span style=\"color:red;\">**Question 6-24:** What is the test accuracy of the experimental model?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3457a68a-e280-4390-a210-e9fa1163aac7",
   "metadata": {
    "id": "3457a68a-e280-4390-a210-e9fa1163aac7"
   },
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3342ff8e-7776-4fbc-b09c-376bb93ca7ff",
   "metadata": {
    "id": "3342ff8e-7776-4fbc-b09c-376bb93ca7ff"
   },
   "source": [
    "## Performance Curves\n",
    "\n",
    "Now, we define plotting functions for the training and validation losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209873aa-ade6-4dea-a9f7-0c5818c41c5f",
   "metadata": {
    "id": "209873aa-ade6-4dea-a9f7-0c5818c41c5f"
   },
   "outputs": [],
   "source": [
    "def plot_training_values(\n",
    "    training_values: List,\n",
    "    validation_values: List,\n",
    "    title: str,\n",
    "    x_label: str,\n",
    "    y_label: str,\n",
    ") -> None:\n",
    "    plt.plot(\n",
    "        np.cumsum(training_values) / np.arange(1, len(training_values) + 1),\n",
    "        label=\"Training\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        np.cumsum(validation_values) / np.arange(1, len(validation_values) + 1),\n",
    "        label=\"Validation\",\n",
    "    )\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc5c216-a94d-4dda-bab0-4bff4829d300",
   "metadata": {
    "id": "0dc5c216-a94d-4dda-bab0-4bff4829d300"
   },
   "source": [
    "Next, we define the function for plotting the loss or accuracy curves to compare the baseline model and the experimental model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab7877-d9ad-42f2-aead-f505af2c052e",
   "metadata": {
    "id": "6aab7877-d9ad-42f2-aead-f505af2c052e"
   },
   "outputs": [],
   "source": [
    "def plot_model_values(\n",
    "    baseline_values: List,\n",
    "    experimental_values: List,\n",
    "    title: str,\n",
    "    x_label: str,\n",
    "    y_label: str,\n",
    ") -> None:\n",
    "    plt.plot(\n",
    "        np.cumsum(baseline_values) / np.arange(1, len(baseline_values) + 1),\n",
    "        label=\"Baseline\",\n",
    "    )  # cumulative moving average\n",
    "    plt.plot(\n",
    "        np.cumsum(experimental_values) / np.arange(1, len(experimental_values) + 1),\n",
    "        label=\"Experimental\",\n",
    "    )\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116c8388-dd8b-4c54-86d3-ea6a0d16fd38",
   "metadata": {
    "id": "116c8388-dd8b-4c54-86d3-ea6a0d16fd38"
   },
   "source": [
    "Then, we plot the training and validation loss curves for the baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbff9b88-d231-456a-bee6-794fe859ed9c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "bbff9b88-d231-456a-bee6-794fe859ed9c",
    "outputId": "eef87fb3-c6c8-4e6b-d42b-d57faf75a911"
   },
   "outputs": [],
   "source": [
    "plot_training_values(\n",
    "    training_values=baseline_train_loss,\n",
    "    validation_values=baseline_validation_loss,\n",
    "    title=\"Baseline Loss Curves\",\n",
    "    x_label=\"Step\",\n",
    "    y_label=\"Loss\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c06d6de-5037-4841-b831-9ff97467b46b",
   "metadata": {
    "id": "6c06d6de-5037-4841-b831-9ff97467b46b"
   },
   "source": [
    "Then, we plot the training and validation accuracy curves for the baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3f602-577e-4fd1-bab0-cc07a08fc621",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "b7d3f602-577e-4fd1-bab0-cc07a08fc621",
    "outputId": "845fbc36-d8ec-41ca-ced9-3ffd6accddca"
   },
   "outputs": [],
   "source": [
    "plot_training_values(\n",
    "    training_values=baseline_train_acc,\n",
    "    validation_values=baseline_validation_acc,\n",
    "    title=\"Baseline Accuracy Curves\",\n",
    "    x_label=\"Step\",\n",
    "    y_label=\"Accuracy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d016fce-3de3-47bb-b5c6-bafd97c63eaa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "4d016fce-3de3-47bb-b5c6-bafd97c63eaa",
    "outputId": "edfa31ce-558c-4782-c7ca-d6728589574d"
   },
   "outputs": [],
   "source": [
    "plot_training_values(\n",
    "    training_values=experimental_train_loss,\n",
    "    validation_values=experimental_validation_loss,\n",
    "    title=\"Experimental Loss Curves\",\n",
    "    x_label=\"Step\",\n",
    "    y_label=\"Loss\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c3a10e-14ee-4515-ae94-fbcb94804914",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "a0c3a10e-14ee-4515-ae94-fbcb94804914",
    "outputId": "c611c75b-d927-4d16-d82f-4fa784e89e30"
   },
   "outputs": [],
   "source": [
    "plot_training_values(\n",
    "    training_values=experimental_train_acc,\n",
    "    validation_values=experimental_validation_acc,\n",
    "    title=\"Experimental Accuracy Curves\",\n",
    "    x_label=\"Step\",\n",
    "    y_label=\"Accuracy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e4eca5-e07c-49cb-849d-c9ea387e92d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "73e4eca5-e07c-49cb-849d-c9ea387e92d6",
    "outputId": "4165766d-916c-4ddc-cd13-cb54805de8a1"
   },
   "outputs": [],
   "source": [
    "plot_model_values(\n",
    "    baseline_train_loss,\n",
    "    experimental_train_loss,\n",
    "    title=\"Training Loss (Baseline vs Experimental)\",\n",
    "    x_label=\"Steps\",\n",
    "    y_label=\"Loss\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054b719-b6aa-4277-ab4f-4af11127f323",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "a054b719-b6aa-4277-ab4f-4af11127f323",
    "outputId": "c3bde633-0f54-4f8a-e8ea-65982920af29"
   },
   "outputs": [],
   "source": [
    "plot_model_values(\n",
    "    baseline_validation_loss,\n",
    "    experimental_validation_loss,\n",
    "    title=\"Validation Loss (Baseline vs Experimental)\",\n",
    "    x_label=\"Steps\",\n",
    "    y_label=\"Loss\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813d6afc-1502-444e-9044-49426b6a7cd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "813d6afc-1502-444e-9044-49426b6a7cd1",
    "outputId": "84305b96-f0dd-49cf-ec93-aeadcb843816"
   },
   "outputs": [],
   "source": [
    "plot_model_values(\n",
    "    baseline_train_acc,\n",
    "    experimental_train_acc,\n",
    "    title=\"Training Accuracy (Baseline vs Experimental)\",\n",
    "    x_label=\"Steps\",\n",
    "    y_label=\"Accuracy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a8ff95-b64b-4986-bb43-a748eb580042",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "c7a8ff95-b64b-4986-bb43-a748eb580042",
    "outputId": "467093bc-59ac-449d-c697-a3e4c67c1174"
   },
   "outputs": [],
   "source": [
    "plot_model_values(\n",
    "    baseline_validation_acc,\n",
    "    experimental_validation_acc,\n",
    "    title=\"Validation Accuracy (Baseline vs Experimental)\",\n",
    "    x_label=\"Steps\",\n",
    "    y_label=\"Accuracy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e2b9b-9336-4967-8055-ffdffce1a297",
   "metadata": {
    "id": "8f5e2b9b-9336-4967-8055-ffdffce1a297"
   },
   "source": [
    "## Ablation on Experimental Model\n",
    "\n",
    "Now, let's play with our experimental model, and try to see if we could further improve its performance.\n",
    "\n",
    "Implement an image classifier neural network with the following specifications,\n",
    "\n",
    "* 2 layers with each layer having 500 neurons each\n",
    "* Each hidden layer will use ReLU function as its activation function\n",
    "* Use SGD as the optimization algorithm with the following learning rates\n",
    "  * $1 \\times 10^{-1}$\n",
    "  * $1 \\times 10^{-2}$\n",
    "  * $1 \\times 10^{-3}$\n",
    "  * $1 \\times 10^{-4}$\n",
    "\n",
    "Train for 50 epochs with mini-batch size 100. This will serve as our challenger or experimental model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3aa906-dc14-409c-aa15-cd0ddba96c30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c3aa906-dc14-409c-aa15-cd0ddba96c30",
    "outputId": "52fa9392-543b-4b56-aebe-98caaa7a04bf"
   },
   "outputs": [],
   "source": [
    "# to do: define the list of learning rates\n",
    "learning_rates = [...]\n",
    "\n",
    "results = dict()\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    # to do: initialize an experimental model for the current learning rate\n",
    "    experimental_model = ...\n",
    "\n",
    "    # to do: switch to GPU\n",
    "    experimental_model = ...\n",
    "\n",
    "    print(f\"Training model with learning rate = {learning_rate}\")\n",
    "\n",
    "    # to do: run the model training for the given learning rate\n",
    "    ...\n",
    "\n",
    "    # to do: evaluate the trained model\n",
    "    ...\n",
    "    \n",
    "    print(f\"Test accuracy: {experimental_test_acc:.4f}\")\n",
    "\n",
    "    # store the results for later retrieval\n",
    "    results[f\"lr-{learning_rate}\"] = dict(\n",
    "        train_acc=experimental_train_acc,\n",
    "        train_loss=experimental_train_loss,\n",
    "        validation_loss=experimental_validation_loss,\n",
    "        validation_acc=experimental_validation_acc,\n",
    "        test_accuracy=experimental_test_acc\n",
    "    )\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee8093c-3ac0-44ce-81c6-0eb0d50d3f84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eee8093c-3ac0-44ce-81c6-0eb0d50d3f84",
    "outputId": "23dba6d8-f43b-45a3-af54-b2b4161b8033"
   },
   "outputs": [],
   "source": [
    "for learning_rate in learning_rates:\n",
    "    plot_training_values(\n",
    "        training_values=results[f\"lr-{learning_rate}\"][\"train_acc\"],\n",
    "        validation_values=results[f\"lr-{learning_rate}\"][\"validation_acc\"],\n",
    "        title=f\"Accuracy Curves (lr={learning_rate})\",\n",
    "        x_label=\"Step\",\n",
    "        y_label=\"Accuracy\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc234798-247f-4b99-a2b6-d80c8028aed5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dc234798-247f-4b99-a2b6-d80c8028aed5",
    "outputId": "f795379d-48c4-429f-f9e0-08d379ecc2af"
   },
   "outputs": [],
   "source": [
    "for learning_rate in learning_rates:\n",
    "    plot_training_values(\n",
    "        training_values=results[f\"lr-{learning_rate}\"][\"train_loss\"],\n",
    "        validation_values=results[f\"lr-{learning_rate}\"][\"validation_loss\"],\n",
    "        title=f\"Loss Curves (lr={learning_rate})\",\n",
    "        x_label=\"Step\",\n",
    "        y_label=\"Loss\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MCTGTF2sR-6i",
   "metadata": {
    "id": "MCTGTF2sR-6i"
   },
   "source": [
    "<span style=\"color:red;\">**Question 6-25:** What was the best test accuracy across the 4 learning rates used for the model? What was the learning rate used?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98LPjJD7SF1A",
   "metadata": {
    "id": "98LPjJD7SF1A"
   },
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ulWtUdTECH",
   "metadata": {
    "id": "f6ulWtUdTECH"
   },
   "source": [
    "<span style=\"color:red;\">**Question 6-26:** Based on the loss curves, what can you say about the model when it was trained with a learning rate of 0.0001?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iEEhHv7NTRkt",
   "metadata": {
    "id": "iEEhHv7NTRkt"
   },
   "source": [
    "**Answers**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0605083-0e06-4d1d-b949-08a9b3f3ac85",
   "metadata": {},
   "source": [
    "### <center>fin</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d5eee9-8cfd-44b9-9194-f6ce226c33d5",
   "metadata": {},
   "source": [
    "\n",
    "<!-- DO NOT MODIFY OR DELETE THIS -->\n",
    "<sup>written by Abien Fred Agarap</sup> <br>\n",
    "<sup>for comments, corrections, suggestions, please email:</sup><sup> abienfred.agarap@gmail.com</sup><br>\n",
    "<!-- DO NOT MODIFY OR DELETE THIS -->"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
